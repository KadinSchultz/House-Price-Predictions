{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Library imports, data imports, and initialisations","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy.stats import norm, probplot\nfrom sklearn.preprocessing import StandardScaler\n\n# Load in the training and testing datasets\ndf_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Impute missing values\nI've chosen to do this first, so that any effects it has on relationships or other aspects of this exploration/modeling can be accounted for early on.","metadata":{}},{"cell_type":"code","source":"def getMissingValues():\n    missingValuesTotal = df_train.isnull().sum().sort_values(ascending = False)\n    missingValuesPercent = (100 * df_train.isnull().sum() / df_train.isnull().count()).sort_values(ascending = False)\n\n    missingValues = pd.concat([missingValuesTotal, missingValuesPercent], axis = 1, keys = ['# Missing', '% Missing'])\n    return missingValues[missingValues['# Missing'] > 0]\n\ngetMissingValues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill values in the training and testing datasets for the column specified\ndef fillMissingValues(var, fill):\n    df_train[var] = df_train[var].fillna(fill)\n    df_test[var] = df_test[var].fillna(fill)\n    \n# PoolQC w/ missing values: 99.66%\n# In data_description.txt, NaN values indicate there is no pool\nfillMissingValues('PoolQC', 'NA')\n\n# MiscFeature w/ missing values: 96.29%\n# In data_description.txt, NaN values indicate there are no misc features\nfillMissingValues('MiscFeature', 'NA')\n\n# Alley w/ missing values: 93.75%\n# In data_description.txt, NaN values indicate there is no alley access\nfillMissingValues('Alley', 'NA')\n\n# Fence w/ missing values: 80.76%\n# In data_description.txt, NaN values indicate there is no fence\nfillMissingValues('Fence', 'NA')\n\n# FireplaceQu w/ missing values: 47.42%\n# In data_description.txt, NaN values indicate there is no fireplace\nfillMissingValues('FireplaceQu', 'NA')\n\n# LotFrontage w/ missing values: 17.8%\n# In data_description.txt, no substitution is given for NaN LotFrontage characteristics; thus I'm going to use the median values for the row's corresponding neighbourhood.\n# This may have issues when a there is a NaN value in a neighbourhood with no other samples, but for now this is not the case.\ndf_train['LotFrontage'] = df_train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# GarageCond, GarageFinish, GarageType, GarageQual, GarageYrBlt w/ missing values: 5.57%;\n# In data_description.txt, NaN values indicate there is no garage\nfor cVar in ['GarageCond', 'GarageType', 'GarageFinish', 'GarageQual']:\n    fillMissingValues(cVar, 'NA')\nfillMissingValues('GarageYrBlt', 0)\n\n# BsmtExposure, BsmtFinType2 w/ missing values: 2.61%; BsmtCond, BsmtFinType1, BsmtQual w/ missing values: 2.54%\n# In data_description.txt, NaN values indicate there is no basement\nfor cVar in ['BsmtExposure', 'BsmtFinType2', 'BsmtCond', 'BsmtFinType1', 'BsmtQual']:\n    fillMissingValues(cVar, 'NA')\n\n# MasVnrArea, MasVnrType w/ missing values: 0.55%\n# In data_description.txt, NaN values indicate there is no masonry vaneer\nfillMissingValues('MasVnrArea', 0)\nfillMissingValues('MasVnrType', 'NA')\n\n# Electrical w/ missing values: 0.07%\n# In data_description.txt, no substitution is given for NaN Electrical characteristics; thus I assume the one missing value is an error to be dropped.\ndf_train = df_train.drop(df_train[df_train['Electrical'].isnull()].index)\n\n# Confirm that all missing values have been fixed\ngetMissingValues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean data types\nChange all categorical columns are as noted as numbers to strings.","metadata":{}},{"cell_type":"code","source":"cols_numToObj = ['MSSubClass', 'OverallQual', 'OverallCond', 'YearBuilt',\n                 'YearRemodAdd', 'GarageYrBlt', 'MoSold', 'YrSold']\n\nfor col in cols_numToObj:\n    df_train[col] = df_train[col].apply(str)\n    df_test[col] = df_test[col].apply(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preliminary correlation inspections\nReview correlation values between each set of characteristics, to identify if there are any redundant columns that could be dropped.","metadata":{}},{"cell_type":"code","source":"# Correlation matrix for all characteristics\ncorrMat = df_train.corr()\n\n# Determine which correlations are significant, and drop the others\n# Assuming correlation values >= 0.8 are significant\ncorrValues = corrMat.unstack().abs()\nc = [corrValues.drop(i, inplace = True) for i, v in corrValues.items()\n     if i[0] == i[1]                           # left and right index are the same\n     or v < 0.8                                # Value is insignificant\n     or (i[1], i[0]) in corrValues.index]      # repeated correlation values\n\n# Sort and print the correlation values\nprint('Characteristics with significant correlations:\\n{}\\n'.format(corrValues.sort_values(ascending = False)))\n\n# Print the figure\nsns.set(font_scale = 1.0, rc = {'figure.figsize': (12, 10)})\nsns.heatmap(corrMat, vmax = 0.8, square = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saleprice correlation matrix\n#     Only including characteristics with the top k correlation values\n\ncorrMin = 0.5\ncorrSalePrice = corrMat[corrMat['SalePrice'].abs() >= corrMin]['SalePrice'].sort_values(ascending = False)\nprint('Characteristics with correlation values over {}:\\n{}\\n'.format(corrMin, corrSalePrice.drop('SalePrice')))\n\ncols = corrMatt.index\nprint(cols)\ncm = np.corrcoef(df_train[cols].values.T)\n\nk = len(corrSalePrice)\n#cols = corrMat.nlargest(k, 'SalePrice')['SalePrice'].index\n#cm = np.corrcoef(df_train[cols].values.T)\n\nsns.set(font_scale = 1.0, rc = {'figure.figsize': (10, 8)})\nsns.heatmap(cm, cbar = True, annot = True, square = True,\n            fmt = '0.2f', annot_kws = {'size': 10}, vmax = 0.8,\n            xticklabels = cols.values, yticklabels = cols.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initial data exploration","metadata":{}},{"cell_type":"code","source":"# Check the size of the datasets\nprint('Size of the training dataset: {}'.format(df_train.shape))\nprint('Size of the testing dataset: {}\\n'.format(df_test.shape))\n\n# Make sure there are no zero-values in the SalePrice column\ndf_train['SalePrice'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no zero-values, which is great for modelling. However, the results above indicate that there will be a right skew.","metadata":{}},{"cell_type":"code","source":"# Fitting params for SalePrice\nprint('Skewness: %f' % df_train['SalePrice'].skew())\nprint('Kurtosis: %f\\n' % df_train['SalePrice'].kurt())\n\n# Sale price histogram\nsns.distplot(df_train['SalePrice'], fit = norm)\nplt.ticklabel_format(style = 'plain', axis = 'y')\nplt.ylabel('Frequency')\n\n# Normal probability plot\nfig = plt.figure()\nres = probplot(df_train['SalePrice'], plot = plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Right skew (positive) confirmed. Noted for later, so that I can apply log transformations to normalise SalePrice.","metadata":{}},{"cell_type":"markdown","source":"Very strong correlations between the following:\n* GarageArea & GarageCars\n* GarageYrBlt & YearBuilt\n* TotRmsAbvGrd & GrLivArea\n* 1stFlrSF & TotalBsmtSF\n\nI'll check which characteristic in each of the above pairs has a stronger correlation with SalePrice, and remove the other column.","metadata":{}},{"cell_type":"markdown","source":"Referring to the previous strong correlations between characteristics (not inc. SalePrice):\n* GarageArea (0.623431) & GarageCars (0.640409)\n* GarageYrBlt (less than 0.5)  & YearBuilt (0.522897)\n* TotRmsAbvGrd (0.533723) & GrLivArea (0.708624)\n* 1stFlrSF (0.605852) & TotalBsmtSF (0.613581)\n\nTherefore the following columns will be removed later:\n* GarageArea\n* GarageYrBlt\n* TotRmsAbvGrd\n* 1stFlrSF\n\nThe following characteristics are worth exploring further:\n* Numerical variables\n  * GrLivArea\n  * GarageCars\n  * TotalBsmtSF\n  * FullBath\n\n* Categorical variables\n  * OverallQual\n  * YearBuilt\n  * YearRemodAdd","metadata":{}},{"cell_type":"code","source":"# Fixing Seaborn's styling after it gets reset by the previous correlation matrices\nsns.set_style('whitegrid')\n\n# Numerical variables\nvariables = ['GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath']\nfor var in variables:\n    plt.scatter(x = df_train[var], y = df_train['SalePrice'])\n    plt.title('Relationship between {} and {}'.format(var, 'SalePrice'))\n    plt.xlabel(var)\n    plt.ylabel('SalePrice')\n    plt.show()\n    \n# Numerical variables\nvariables = ['OverallQual', 'YearBuilt', 'YearRemodAdd']\nfor var in variables:\n    sns.boxplot(x = df_train[var], y = df_train['SalePrice'])\n    plt.title('Relationship between {} and {}'.format(var, 'SalePrice'))\n    plt.xlabel(var)\n    plt.ylabel('SalePrice')\n    plt.xticks(rotation = 90)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The relationships are as expected, though there are a handful of outliers that will likely need to be dealt with. \n\n* GrLivArea and TotalBsmtSF have obvious outliers to the right of the plot, these will be excluded\n* YearBuilt and YearRemodAdd both have outliers near the top of the plot, these may be excluded\n\nI shall remove these outliers, but ignore outliers in characteristics not mentioned above (because their correlation to SalePrice is much lower, their outliers shouldn't have as much negative impact).","metadata":{}},{"cell_type":"code","source":"originalLen = len(df_train)\n\n# Two outliers within the GrLivArea characteristic\ndf_train = df_train.drop(df_train[(df_train['GrLivArea'] > 4500) & (df_train['SalePrice'] < 200000)].index)\n\n# One outlier within the TotalBsmtSF characteristic\ndf_train = df_train.drop(df_train[(df_train['TotalBsmtSF'] > 6000) & (df_train['SalePrice'] < 200000)].index)\n\n# Three outliers within the YearBuilt characteristic\ndf_train = df_train.drop(df_train[(df_train['YearBuilt'] < 2000) & (df_train['SalePrice'] > 600000)].index)\n\n# Three outliers within the YearRemodAdd characteristic\ndf_train = df_train.drop(df_train[(df_train['YearRemodAdd'] < 2000) & (df_train['SalePrice'] > 600000)].index)\n\nfinalLen = len(df_train)\nprint('Original training size: {}\\nNew training size: {}\\n'.format(originalLen, finalLen))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":".\n.\n.\n.\n.\n.\n.\n.\n.\n.\n","metadata":{}}]}