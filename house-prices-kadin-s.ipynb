{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Library imports, data imports, and initialisations","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import AnchoredText\n%matplotlib inline\n\nimport seaborn as sns\ncolor = sns.color_palette()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy.special import boxcox1p\nfrom scipy.stats import norm, probplot\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load in datasets\nRead the CSV files for the training and testing data, then create a concatenated version for feature engineering.","metadata":{}},{"cell_type":"code","source":"# Load in the training and testing datasets\ndf_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n# Save and remove the ID columns\ntrainingIDs = df_train['Id']\ntestingIDs = df_test['Id']\n\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\n\n# Combine the datasets\ndf_allData = pd.concat((df_train, df_test)).reset_index(drop = True)\ndf_allData.drop(['SalePrice'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Impute missing values\nI've chosen to do this first, so that any effects it has on relationships or other aspects of this exploration/modeling can be accounted for early on.","metadata":{}},{"cell_type":"code","source":"def getMissingValues():\n    missingValuesTotal = df_allData.isnull().sum().sort_values(ascending = False)\n    missingValuesPercent = (100 * df_allData.isnull().sum() / df_allData.isnull().count()).sort_values(ascending = False)\n\n    missingValues = pd.concat([missingValuesTotal, missingValuesPercent], axis = 1, keys = ['# Missing', '% Missing'])\n    return missingValues[missingValues['# Missing'] > 0]\n\ngetMissingValues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill values in the training and testing datasets for the column specified\ndef fillMissingValues(var, fill):\n    #df_train[var] = df_train[var].fillna(fill)\n    #df_test[var] = df_test[var].fillna(fill)\n    df_allData[var] = df_allData[var].fillna(fill)\n    \n# PoolQC w/ missing values: 99.66%\n# In data_description.txt, NaN values indicate there is no pool\nfillMissingValues('PoolQC', 'NA')\n\n# MiscFeature w/ missing values: 96.29%\n# In data_description.txt, NaN values indicate there are no misc features\nfillMissingValues('MiscFeature', 'NA')\n\n# Alley w/ missing values: 93.75%\n# In data_description.txt, NaN values indicate there is no alley access\nfillMissingValues('Alley', 'NA')\n\n# Fence w/ missing values: 80.76%\n# In data_description.txt, NaN values indicate there is no fence\nfillMissingValues('Fence', 'NA')\n\n# FireplaceQu w/ missing values: 47.42%\n# In data_description.txt, NaN values indicate there is no fireplace\nfillMissingValues('FireplaceQu', 'NA')\n\n# LotFrontage w/ missing values: 17.8%\n# In data_description.txt, no substitution is given for NaN LotFrontage characteristics; thus I'm going to use the median values for the row's corresponding neighbourhood.\n# This may have issues when a there is a NaN value in a neighbourhood with no other samples, but for now this is not the case.\ndf_allData['LotFrontage'] = df_allData.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# GarageCond, GarageFinish, GarageType, GarageQual, GarageYrBlt w/ missing values: 5.57%;\n# In data_description.txt, NaN values indicate there is no garage\nfor cVar in ['GarageCond', 'GarageType', 'GarageFinish', 'GarageQual']:\n    fillMissingValues(cVar, 'NA')\nfillMissingValues('GarageYrBlt', 0)\n\n# BsmtExposure, BsmtFinType2 w/ missing values: 2.61%; BsmtCond, BsmtFinType1, BsmtQual w/ missing values: 2.54%\n# In data_description.txt, NaN values indicate there is no basement\nfor cVar in ['BsmtExposure', 'BsmtFinType2', 'BsmtCond', 'BsmtFinType1', 'BsmtQual']:\n    fillMissingValues(cVar, 'NA')\n\n# MasVnrArea, MasVnrType w/ missing values: 0.55%\n# In data_description.txt, NaN values indicate there is no masonry vaneer\nfillMissingValues('MasVnrArea', 0)\nfillMissingValues('MasVnrType', 'NA')\n\n# Electrical w/ missing values: 0.07%\n# In data_description.txt, no substitution is given for NaN Electrical characteristics; thus I assume the one missing value is an error to be dropped.\ndf_allData = df_allData.drop(df_allData[df_allData['Electrical'].isnull()].index)\n\n# Confirm that all missing values have been fixed\ngetMissingValues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean data types\nChange all categorical columns are as noted as numbers to strings.","metadata":{}},{"cell_type":"code","source":"cols_numToObj = ['MSSubClass', 'OverallQual', 'OverallCond', 'YearBuilt',\n                 'YearRemodAdd', 'GarageYrBlt', 'MoSold', 'YrSold']\n\nfor col in cols_numToObj:\n    df_train[col] = df_train[col].apply(str)\n    df_test[col] = df_test[col].apply(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preliminary correlation inspections\nReview correlation values between each set of characteristics, to identify if there are any redundant columns that could be dropped.","metadata":{}},{"cell_type":"code","source":"# Correlation matrix for all characteristics\ncorrMat = df_train.corr()\n\n# Determine which correlations are significant, and drop the others\n# Assuming correlation values >= 0.8 are significant\ncorrValues = corrMat.unstack().abs()\nc = [corrValues.drop(i, inplace = True) for i, v in corrValues.items()\n     if i[0] == i[1]                           # left and right index are the same\n     or v < 0.8                                # Value is insignificant\n     or (i[1], i[0]) in corrValues.index]      # repeated correlation values\n\n# Sort and print the correlation values\nprint('Characteristics with significant correlations:\\n{}\\n'.format(corrValues.sort_values(ascending = False)))\n\n# Print the figure\nsns.set(font_scale = 1.0, rc = {'figure.figsize': (12, 10)})\nsns.heatmap(corrMat, vmax = 0.8, square = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Very strong correlations between the following:\n* GarageArea & GarageCars\n* TotRmsAbvGrd & GrLivArea\n* 1stFlrSF & TotalBsmtSF\n\nI'll check which characteristic in each of the above pairs has a stronger correlation with SalePrice, and remove the other column.","metadata":{}},{"cell_type":"code","source":"# Saleprice correlation matrix\n#     Only including characteristics with correlation values over corrMin (relative to SalePrice)\n\ncorrMin = 0.5\ncorrSalePrice = corrMat[corrMat['SalePrice'].abs() >= corrMin]['SalePrice'].sort_values(ascending = False)\nprint('Characteristics with correlation values over {}:\\n{}\\n'.format(corrMin, corrSalePrice.drop('SalePrice')))\n\ncols = corrSalePrice.index\ncm = np.corrcoef(df_train[cols].values.T)\n\nsns.set(font_scale = 1.0, rc = {'figure.figsize': (10, 8)})\nsns.heatmap(cm, cbar = True, annot = True, square = True,\n            fmt = '0.2f', annot_kws = {'size': 10}, vmax = 0.8,\n            xticklabels = cols.values, yticklabels = cols.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Referring to the previous strong correlations between characteristics (not inc. SalePrice):\n* GarageArea (0.623423) & GarageCars (0.640473)\n* TotRmsAbvGrd (0.533779) & GrLivArea (0.708618)\n* 1stFlrSF (0.605968) & TotalBsmtSF (0.613905)\n\nTherefore the following columns will be removed:\n* GarageArea\n* TotRmsAbvGrd\n* 1stFlrSF","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop(columns = ['GarageArea', 'TotRmsAbvGrd', '1stFlrSF'])\ndf_test = df_test.drop(columns = ['GarageArea', 'TotRmsAbvGrd', '1stFlrSF'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the following characteristics are worth exploring further:\n* GrLivArea\n* GarageCars\n* TotalBsmtSF\n* FullBath","metadata":{}},{"cell_type":"markdown","source":"## Initial data exploration","metadata":{}},{"cell_type":"code","source":"# Check the size of the datasets\nprint('Size of the training dataset: {}'.format(df_train.shape))\nprint('Size of the testing dataset: {}\\n'.format(df_test.shape))\n\n# Make sure there are no zero-values in the SalePrice column\ndf_train['SalePrice'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no zero-values, which is great for modelling. However, the results above indicate that there will be a right skew.","metadata":{}},{"cell_type":"code","source":"# Making these into functions for use later one\n\n# Generate histogram, and print values for skewness and kurtosis on it\ndef histogramPlot(var, ax = None):\n    p = 0\n    if ax is None:\n        ax = plt.gca()\n        p = 1\n    f = sns.distplot(df_train[var], fit = norm, ax = ax)\n    f.ticklabel_format(style = 'plain', axis = 'y')\n    ax.set(title = 'Distribution plot for {} values'.format(var),\n            ylabel = 'Frequency')\n    ax.add_artist(AnchoredText('Skewness: {:.4f}\\nKurtosis: {:.4f}\\n'.format(df_train[var].skew(), df_train[var].kurt()),\n                                loc = 'upper right', frameon = False))\n    \n    # If not part of a subplot, print the figure\n    if p == 1: plt.show()\n\n    \n# Generate probability plot (QQ-plot)\ndef probabilityPlot(var, ax = None):\n    p = 0\n    if ax is None:\n        ax = plt.gca()\n        p = 1\n    f = probplot(df_train[var], plot = plt)\n    ax.set(title = 'Probability plot for {} values'.format(var))\n    \n    # If not part of a subplot, print the figure\n    if p == 1: plt.show()\n\n\n# Print the histogram abd probability plots as a side-by-side subplot\ndef histAndProbPlot(var):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 6))\n    plt.subplots_adjust(wspace = 0.4)\n    histogramPlot(var, ax1)\n    probabilityPlot(var, ax2)\n    plt.show()\n\n    \nhistAndProbPlot('SalePrice')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Right skew (positive) confirmed. Noted for later, so that I can apply log transformations to normalise SalePrice (and likely other numeric characteristics).","metadata":{}},{"cell_type":"code","source":"# Making this into a function for use later one\ndef scatterPlot(xVar, yVar):\n    plt.scatter(x = df_train[xVar], y = df_train[yVar])\n    plt.title('Relationship between {} and {}'.format(xVar, yVar))\n    plt.xlabel(xVar)\n    plt.ylabel(yVar)\n    plt.show()\n    \n# Numerical values\nvariables = ['GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath']\nsns.set(font_scale = 1.0, rc = {'figure.figsize': (5, 5)})\nfor var in variables:\n    scatterPlot(var, 'SalePrice')\n    \n# Categorical variables\n#variables = ['OverallQual', 'YearBuilt', 'YearRemodAdd']\n#for var in variables:\n#    sns.boxplot(x = df_train[var], y = df_train['SalePrice'])\n#    plt.title('Relationship between {} and {}'.format(var, 'SalePrice'))\n#    plt.xlabel(var)\n#    plt.ylabel('SalePrice')\n#    plt.xticks(rotation = 90)\n#    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The relationships are as expected, though there are some outliers to remove. Whilst removing data can be hazardous, GrLivArea and TotalBsmtSF have obvious outliers to the right of the plot, and these will be excluded. However, I'll ignore outliers in characteristics not mentioned above (because their correlation to SalePrice is much lower, their outliers shouldn't have as much negative impact).","metadata":{}},{"cell_type":"code","source":"# Two outliers within the GrLivArea characteristic\ndf_train = df_train.drop(df_train[(df_train['GrLivArea'] > 4500) & (df_train['SalePrice'] < 200000)].index)\n\n# One outlier within the TotalBsmtSF characteristic\ndf_train = df_train.drop(df_train[(df_train['TotalBsmtSF'] > 6000) & (df_train['SalePrice'] < 200000)].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardising skewed features","metadata":{}},{"cell_type":"code","source":"# Numeric columns\ntypes = df_train.dtypes[df_train.dtypes != 'object']\n#types = types[~types.index.isin(['Id'])].index\n\n# Skews in the training data\n#varSkews = pd.Series([df_train[t].skew() for t in types], types).sort_values(ascending=False)\n\n# Apply log transformation to highly skewed features (skew > 0.75)\n#varToTransform = varSkews[abs(varSkews) > 0.75].index\n#for var in varToTransform:\n#    df_train[var] = np.log1p(df_train[var])\n#    df_test[var] = np.log1p(df_test[var])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}